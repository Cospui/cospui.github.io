<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Xiaoyu Chen</title>

    <meta name="author" content="Xiaoyu Chen">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Xiaoyu Chen
                </p>
                <p>
                  I am Ph.D. student in at <a href="https://iiis.tsinghua.edu.cn/en/">IIIS, Tsinghua University</a>, I am fortunate to be advised by <a href="http://people.iiis.tsinghua.edu.cn/~jychen/">Prof. Jianyu Chen</a>, and work closely with <a href="https://www.microsoft.com/en-us/research/people/lizo/">Dr. Li Zhao</a> at Microsoft Research. 
                </p>
                <p>
                  Prior to that, I received dual bachelor's degrees in <a href="https://www.cs.tsinghua.edu.cn/csen/">Computer Science and Technology</a> and <a href="https://www.sem.tsinghua.edu.cn/en/">Economics</a> (second degree) from Tsinghua University. My research has been recognized with awards including outstanding graduate student honors, outstanding undergraduate thesis award, and a national scholarship.
                </p>
                <p style="text-align:center">
                  <a href="mailto:chen-xy21@mails.tsinghua.edu.cn">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=ziGzPNgAAAAJ&hl=en">Scholar</a>
                </p>
              </td>
              <td style="padding:2.5%;width:30%;max-width:30%">
                <img style="width:100%;max-width:100%" alt="profile photo" src="images/xy_photo.jpg" class="hoverZoomLink">
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research endeavors to develop human-like embodied agents, specializing in foundation models for embodied decision making and Reinforcement Learning. 
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                        
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/IGOR.png" alt="IGOR" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://aka.ms/project-igor"><span class="papertitle">IGOR: Image-GOal Representations are the Atomic Control Units for Foundation Models in Embodied AI</span></a>
                <br>
                <strong>Xiaoyu Chen</strong>*,
                Junliang Guo*,
                Tianyu He*,
                Chuheng Zhang*,
                Pushi Zhang,
                Derek Cathera Yang,
                Li Zhao*,
                Jiang Bian
                <br>
                <em>arxiv</em>
                <br>
                <p><font color="red"><strong>New!</strong></font>  We introduce IGOR, a framework that learns latent actions from Internet-scale videos that enable cross-embodiment and cross-task generalization.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/HIRT.png" alt="HIRT" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2410.05273"><span class="papertitle">HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers</span></a>
                <br>
                Jianke Zhang,
                Yanjiang Guo,
                <strong>Xiaoyu Chen</strong>,
                Yen-Jen Wang,
                Yucheng Hu,
                Chengming Shi,
                Jianyu Chen
                <br>
                <em>CoRL 2024</em>
                <br>
                <p>A Hierarchical Robot Transformer framework that enables flexible frequency and performance trade-off.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/PAD.png" alt="PAD" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://sites.google.com/view/pad-paper"><span class="papertitle">Prediction with Action: Visual Policy Learning via Joint Denoising Process</span></a>
                <br>
                Yanjiang Guo,
                Yucheng Hu,
                Jianke Zhang,
                Yen-Jen Wang,
                <strong>Xiaoyu Chen</strong>,
                Chaochao Lu,
                Jianyu Chen
                <br>
                <em>NeurIPS 2024</em>
                <br>
                <p>A novel framework to predict images and robot actions through joint dinoising process.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/ABA_image.png" alt="ABA" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2305.15695"><span class="papertitle">Asking Before Acting: Gather Information in Embodied Decision Making with Language Models</span></a>
                <br>
                <strong>Xiaoyu Chen</strong>,
                Shenao Zhang,
                Pushi Zhang,
                Li Zhao,
                Jianyu Chen
                <br>
                <em>Preprint</em>
                <br>
                <p>A simple yet effective method for embodied agents to proactively gather information before acting, minimizing unnecessary exploration and mistakes.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/SeCBAD_image.png" alt="SeCBAD" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2212.12735"><span class="papertitle">An Adaptive Deep RL Method for Non-Stationary Environments with Piecewise Stable Context</span></a>
                <br>
                <strong>Xiaoyu Chen</strong>*,
                Xiangming Zhu*, 
                Yufeng Zheng, 
                Pushi Zhang, 
                Li Zhao,
                Wenxue Cheng, 
                Peng Cheng, 
                Yongqiang Xiong, 
                Tao Qin, 
                Jianyu Chen,
                Tie-Yan Liu
                <br>
                <em>NeurIPS 2022</em>
                <br>
                <p>A new RL method (SeCBAD) for handling real-world situations where the environment changes abruptly within an episode , allowing agents to adapt to these context variations.</p>
              </td>
            </tr>
          
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/FORBES_image.png" alt="FORBES" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2205.11051"><span class="papertitle">Flow-based Recurrent Belief State Learning for POMDPs</span></a>
                <br>
                <strong>Xiaoyu Chen</strong>,
                Yao Mu,
                Ping Luo,
                Shengbo Li,
                Jianyu Chen
                <br>
                <em>ICML 2022</em> &nbsp <font color="red"><strong>(Outstanding Undergraduate Thesis Award)</strong></font>
                <br>
                <p>A new method (FORBES) for learning general continuous belief states in POMDPs, enhances the performance of downstream RL algorithms by reducing approximation errors during state inference.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/MD3QN_image.png" alt="MD3QN" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2110.13578"><span class="papertitle">Distributional Reinforcement Learning for Multi-Dimensional Reward Functions</span></a>
                <br>
                Pushi Zhang*,
                <strong>Xiaoyu Chen</strong>*,
                Li Zhao,
                Wei Xiong,
                Tao Qin,
                Tie-Yan Liu
                <br>
                <em>NeurIPS 2021</em>
                <br>
                <p>Model the joint return distribution across multiple reward sources in distributional RL, capturing both inherent randomness and rich correlations.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/ORDC_image.png" alt="ORDC" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2307.11685"><span class="papertitle">Towards Generalizable Reinforcement Learning for Trade Execution</span></a>
                <br>
                Chuheng Zhang,
                Yitong Duan,
                <strong>Xiaoyu Chen</strong>,
                Jianyu Chen,
                Jian Li,
                Li Zhao
                <br>
                <em>IJCAI 2023</em>
                <br>
                <p>Investigate and mitigate overfitting issues that arise when applying reinforcement learning to optimized trade execution.</p>
              </td>
            </tr>


            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/NA_image.png" alt="NoisyAgent" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2007.13729"><span class="papertitle">Noisy Agents: Self-supervised Exploration by Predicting Auditory Events</span></a>
                <br>
                Chuang Gan,
                <strong>Xiaoyu Chen</strong>,
                Phillip Isola,
                Antonio Torralba,
                Joshua B. Tenenbaum
                <br>
                <em>IROS 2022</em>
                <br>
                <p> A novel type of intrinsic motivation that encourages the agent to understand the causal effect of its actions through auditory event prediction.</p>
              </td>
            </tr>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/RDP_image.png" alt="RDP" width="160" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2107.01561"><span class="papertitle">Certifiably Robust Interpretation via Renyi Differential Privacy</span></a>
                <br>
                Ao Liu,
                <strong>Xiaoyu Chen</strong>,
                Sijia Liu,
                Lirong Xia,
                Chuang Gan
                <br>
                <em>AAAI 2023 journal track</em>
                <br>
                <p> A new interpretation method for convolutional neural networks (CNNs) based on Rényi differential privacy.</p>
              </td>
            </tr>
            

          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Teaching and Service</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <ul>
                  <li>TAs:
                    <ul>
                      <li>🤖 Introduction to Robotics, Summer 2021</li>
                      <li>🖥️ Intelligent System and Robotics, Fall 2021</li>
                    </ul>
                  </li>
                  <li>Actively contributing as reviewers for ICML, NeurIPS and ICLR.</li>
                </ul>
              </tr>
            </tbody></table>
            
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  This website is taken from <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
